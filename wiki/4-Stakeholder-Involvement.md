# Stakeholder Involvement

AI evaluation requires diverse perspectives. Here's who to involve and when.

## Key Stakeholders

### Domain Experts

**Role:** Validate correctness, provide ground truth

**When:**
- Pre-launch: Define "good" examples, create rubrics
- Testing: Evaluate sample outputs
- Post-launch: Audit concerning outputs

**How to Engage:**
- Review sessions (show outputs, get feedback)
- Rubric creation workshops
- Regular audits (monthly/quarterly)

**Example:**
- Medical AI → Doctors review diagnoses
- Legal AI → Lawyers validate contract analysis

---

### End Users

**Role:** Evaluate usefulness, UX

**When:**
- Pilot testing
- Beta programs
- Ongoing feedback

**How to Engage:**
- User surveys (CSAT, NPS)
- Interview sessions
- Usage tracking
- In-app feedback (thumbs up/down)

**Example:**
- Support AI → Agents rate helpfulness
- Code assistant → Developers acceptance/rejection

---

### Business Stakeholders

**Role:** Define metrics, prioritize, make decisions

**When:**
- Upfront: Goal setting, KPI definition
- Regular reviews: Metric dashboards
- Decision points: Launch readiness

**How to Engage:**
- KPI alignment meetings
- Executive dashboards
- Business reviews (monthly/quarterly)
- ROI analysis

**Example:**
- VP Support → Cost savings targets
- CFO → Budget and ROI review

---

### Engineering/Product

**Role:** Implement, iterate, fix issues

**When:**
- Throughout entire process
- Daily during development
- On-call for production issues

**How to Engage:**
- Regular syncs on metrics
- Incident reviews
- Retrospectives
- Access to logs and metrics

---

## Engagement Plan Template

```
PRE-LAUNCH:
□ Domain experts: Define examples, rubrics
□ Business: Align on KPIs
□ Users: Participate in pilot
□ Engineering: Build evaluation infrastructure

DURING TESTING:
□ Domain experts: Rate 50-100 samples/week
□ Users: Provide feedback
□ Business: Review weekly metrics
□ Engineering: Monitor, fix issues

POST-LAUNCH:
□ Domain experts: Monthly audits
□ Users: Ongoing surveys
□ Business: Quarterly reviews
□ Engineering: Continuous monitoring
```

---

## Feedback Collection Methods

### Quantitative
- Thumbs up/down
- 1-5 star ratings
- Likert scales
- NPS/CSAT scores
- Usage metrics

### Qualitative
- Open-text feedback
- User interviews (30-60 min)
- Support ticket analysis
- Social media monitoring
- Usability testing

---

## Best Practices

**Make feedback easy:**
- One-click ratings
- Minimal friction
- In-context collection

**Close the loop:**
- Show users their impact
- Communicate changes
- Thank contributors

**Act on feedback:**
- Visible improvements
- Regular updates
- Transparent roadmap

**Regular cadence:**
- Weekly or monthly
- Scheduled reviews
- Not just ad-hoc

---

## Common Mistakes

❌ **Only involving engineers**
- Missing domain expertise
- Lack of user perspective

✅ **Diverse stakeholder group from day one**

---

❌ **Asking for feedback but never acting**
- Stakeholder fatigue
- Loss of trust

✅ **Close feedback loops, show impact**

---

❌ **One-time engagement**
- Evaluation as checkbox
- No ongoing involvement

✅ **Continuous engagement throughout lifecycle**

---

## Next Steps

**Plan your iteration strategy:**
- [Iteration & Improvement →](5-Iteration-Improvement.md)
- [Templates & Checklists →](7-Templates-Checklists.md)

**See examples:**
- [System Types & Examples →](6-System-Types-Examples.md)

---

[← Evaluation Strategies](3-Evaluation-Strategies.md) | [Next: Iteration & Improvement →](5-Iteration-Improvement.md)
